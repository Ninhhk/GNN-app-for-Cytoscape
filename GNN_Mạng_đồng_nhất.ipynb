{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "7fbYZD53LHGp"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install torch-geometric\n",
        "# !pip install py4cytoscape seaborn\n",
        "# !pip install scikit-learn #dim reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gFYwOV1bbJqK",
        "outputId": "acfc8486-b4f5-473a-a744-909e4dcf3f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Javascript client ... 2bad58b3-61ac-4b79-81e6-54c251f39a3b on https://jupyter-bridge.cytoscape.org\n"
          ]
        },
        {
          "data": {
            "application/javascript": "var Channel = \"2bad58b3-61ac-4b79-81e6-54c251f39a3b\"; \n\nvar JupyterBridge = \"https://jupyter-bridge.cytoscape.org\"; \n\n  /*\n    These functions serve as a connector between a remote Jupyter server and Cytoscape.\n    They run in the user's browser, which also shows the Jupyter Notebook.\n\n    A remote Jupyter Notebook call to the py4cytoscape package is forwarded to the Jupyter Bridge,\n    which is a standalone server. The functions in this connector execute in the Jupyter Notebook\n    browser, which executes on the same PC as Cytoscape. So, that's 4 components: (A) remote\n    Jupyter Notebook, (B) separate Jupyter Bridge server, (C) this browser-based component, and\n    (D) Cytoscape. (A) is on a remote server, (B) is on a different remote server, and (C) and (D)\n    are on the user's PC.\n\n    (A) calls its py4cytoscape module, which forwards the request (in a JSON wrapper) to (B).\n    (C) picks up the request from (B), unpacks the request and forwards it to (D). (C) awaits a\n    reply from (D), and when it gets it, it forwards the reply (in a JSON wrapper) to (B).\n    (A)'s py4cytoscape module picks up the reply on (B) when it becomes available, unpacks it,\n    and returns it to (A).\n\n    A Jupyter Notebook can talk to only one Cytoscape (i.e., the one on the machine running the\n    Jupyter Notebook browser), and Cytoscape should be called by only one Jupyter Notebook. The\n    Jupyter Bridge differentiates between Notebook-Cytoscape conversations via a channel UUID.\n    The UUID is prepended to this browser component by py4Cytoscape, and the component is\n    started by the Jupyter Notebook. (I wish py4Cytoscape could start the component, too, but I\n    haven't figured out how to do that yet, so startup code *is* required in the Jupyter\n    Notebook.)\n\n    Note that for the case of a Jupyter server running on the same machine as Cytoscape, this\n    bridge isn't necessary because the Jupyter server's HTTP calls can easily connect to\n    Cytoscape over a localhost socket. So, the combination of Jupyter Bridge and this browser\n    component solves the problem of a Jupyter server (e.g., Google's Colab) that can't\n    connect to Cytoscape that sits behind a firewall.\n\n    The request represents an HTTP call that py4cytoscape would normally make via HTTP directly\n    to Cytoscape via localhost when both py4cytoscape and Cytoscape are running on the same machine.\n */\n\nconst VERSION = '0.0.2'\n\nvar showDebug; // Flag indicating whether to show Jupyter-bridge progress\nif (typeof showDebug === 'undefined') {\n    showDebug = false\n}\nif (showDebug) {\n    alert(\"Starting Jupyter-bridge browser component\")\n}\n\n//const JupyterBridge = 'http://127.0.0.1:5000' // for testing against local Jupyter-bridge\nvar JupyterBridge; // URL of Jupyter-bridge server could be defined by assignment pre-pended to this file\nif (typeof JupyterBridge === 'undefined') {\n    JupyterBridge = 'https://jupyter-bridge.cytoscape.org' // for production\n}\nvar Channel; // Unique constant that could be defined by assignment pre-pended to this file\nif (typeof Channel === 'undefined') { // ... but if not assigned, use a debugging value\n    Channel = 1\n}\n\n\nvar httpR = new XMLHttpRequest(); // for sending reply to Jupyter-bridge\nvar httpRE = new XMLHttpRequest(); // for sending backup error reply to Jupyter-bridge\nvar httpC = new XMLHttpRequest(); // for sending command to Cytoscape\nvar httpJ = new XMLHttpRequest(); // for fetching request from Jupyter-bridge\n\nconst HTTP_OK = 200\nconst HTTP_SYS_ERR = 500\nconst HTTP_TIMEOUT = 408\nconst HTTP_TOO_MANY = 429\n\n\n /* This function is useful if we want to rewrite the incoming URL to resolve just to our local one.\n    Doing this stops the Jupyter component from abusing this client to call out to endpoints other\n    than local Cytoscape. On the other hand, it makes it hard to detect when the Jupyter component\n    has specified a genuinely bad URL and really should get an error result. For now, we'll execute\n    the Jupyter-supplied URL and return the result, whatever it may be.\n\nconst LocalCytoscape = 'http://127.0.0.1:1234'\n\nfunction parseURL(url) {\n    var reURLInformation = new RegExp([\n        '^(https?:)//', // protocol\n        '(([^:/?#]*)(?::([0-9]+))?)', // host (hostname and port)\n        '(/{0,1}[^?#]*)', // pathname\n        '(\\\\?[^#]*|)', // search\n        '(#.*|)$' // hash\n    ].join(''));\n    var match = url.match(reURLInformation);\n    return match && {\n        url: url,\n        protocol: match[1],\n        host: match[2],\n        hostname: match[3],\n        port: match[4],\n        pathname: match[5],\n        search: match[6],\n        hash: match[7]\n    }\n}\n*/\n\nfunction replyCytoscape(replyStatus, replyStatusText, replyText) {\n\n    // Clean up after Jupyter bridge accepts reply\n    httpR.onreadystatechange = function() {\n        if (httpR.readyState === 4) {\n            if (showDebug) {\n                console.log(' status from queue_reply: ' + httpR.status + ', reply: ' + httpR.responseText)\n            }\n        }\n    }\n\n    httpR.onerror = function() {\n        // Clean up after Jupyter bridge accepts backup reply\n        httpRE.onreadystatechange = function() {\n            if (httpRE.readyState === 4) {\n                if (showDebug) {\n                    console.log(' status from backup queue_reply: ' + httpRE.status + ', reply: ' + httpRE.responseText)\n                }\n            }\n        }\n\n        if (showDebug) {\n            console.log(' error from queue_reply -- could be Jupyter-Bridge server reject')\n        }\n        var errReply = {'status': HTTP_SYS_ERR, 'reason': 'Jupyter-Bridge rejected reply', 'text': 'Possibly reply is too long for Jupyter-Bridge server'}\n        httpRE.open('POST', jupyterBridgeURL, true)\n        httpRE.setRequestHeader('Content-Type', 'text/plain')\n        httpRE.send(JSON.stringify(errReply))\n    }\n\n    var reply = {'status': replyStatus, 'reason': replyStatusText, 'text': replyText}\n\n    // Send reply to Jupyter bridge\n    var jupyterBridgeURL = JupyterBridge + '/queue_reply?channel=' + Channel\n    if (showDebug) {\n        console.log('Starting queue to Jupyter bridge: ' + jupyterBridgeURL)\n    }\n    httpR.open('POST', jupyterBridgeURL, true)\n    httpR.setRequestHeader('Content-Type', 'text/plain')\n    httpR.send(JSON.stringify(reply))\n}\n\nfunction callCytoscape(callSpec) {\n\n    // Captures Cytoscape reply and sends it on\n    httpC.onreadystatechange = function() {\n        if (httpC.readyState === 4) {\n            if (showDebug) {\n                console.log(' status from CyREST: ' + httpC.status + ', statusText: ' + httpC.statusText + ', reply: ' + httpC.responseText)\n            }\n            // Note that httpC.status is 0 if the URL can't be reached *OR* there is a CORS violation.\n            // I wish I could tell the difference because for a CORS violation, I'd return a 404,\n            // which would roughly match what Python's native request package would return.\n            // The practical consequence is that the ultimate caller (e.g., py4cytoscape)\n            // returns different exceptions, depending on wither this module is doing the\n            // HTTP operation or the native Python requests package is. This is minor, but\n            // messes up tests that verify the exception type.\n            replyCytoscape(httpC.status, httpC.statusText, httpC.responseText)\n            waitOnJupyterBridge()\n        }\n    }\n\n//  Build up request to Cytoscape, making sure host is local.\n//    Too heavy handed: localURL = LocalCytoscape + parseURL(callSpec.url).pathname\n    var localURL = callSpec.url // Try using what was passed in ... is there a security risk??\n\n    if (showDebug) {\n        console.log('Command to CyREST: ' + callSpec.command + ' (' + localURL + ')')\n        if (callSpec.params) {\n            console.log(' params: ' + JSON.stringify(callSpec.params))\n        }\n        if (callSpec.headers) {\n            console.log(' header: ' + JSON.stringify(callSpec.headers))\n        }\n        if (callSpec.data) {\n            console.log('   data: ' + JSON.stringify(callSpec.data))\n        }\n    }\n\n    if (callSpec.command === 'webbrowser') {\n        if (window.open(callSpec.url)) {\n            replyCytoscape(HTTP_OK, 'OK', '')\n        } else {\n            replyCytoscape(HTTP_SYS_ERR, 'BAD BROWSER OPEN', '')\n        }\n        waitOnJupyterBridge()\n    } else if (callSpec.command === 'version') {\n        replyCytoscape(HTTP_OK, 'OK',\n            JSON.stringify({\"jupyterBridgeVersion\": VERSION}))\n        waitOnJupyterBridge()\n    } else {\n        var joiner = '?'\n        for (let param in callSpec.params) {\n            localURL = localURL + joiner + param + '=' + encodeURIComponent(callSpec.params[param])\n            joiner = '&'\n        }\n\n        httpC.open(callSpec.command, localURL, true)\n        for (let header in callSpec.headers) {\n            httpC.setRequestHeader(header, callSpec.headers[header])\n        }\n\n        // Send request to Cytoscape ... reply goes to onreadystatechange handler\n        httpC.send(JSON.stringify(callSpec.data))\n    }\n}\n\nfunction waitOnJupyterBridge() {\n\n    // Captures request from Jupyter bridge\n    httpJ.onreadystatechange = function() {\n        if (httpJ.readyState === 4) {\n            if (showDebug) {\n                console.log(' status from dequeue_request: ' + httpJ.status + ', reply: ' + httpJ.responseText)\n            }\n            try {\n                if (httpJ.status == HTTP_TOO_MANY) {\n                    // Nothing more to do ... the browser has created too many listeners,\n                    // and it's time to stop listening because the server saw a listener\n                    // listening on this channel before we got there.\n                    console.log('  shutting down because of redundant reader on channel: ' + Channel)\n                } else {\n                    if (httpJ.status === HTTP_TIMEOUT) {\n                        waitOnJupyterBridge()\n                    } else {\n                        callCytoscape(JSON.parse(httpJ.responseText))\n                    }\n                }\n            } catch(err) {\n                if (showDebug) {\n                    console.log(' exception calling Cytoscape: ' + err)\n                }\n                // Bad responseText means something bad happened that we don't understand.\n                // Go wait on another request, as there's nothing to call Cytoscape with.\n                waitOnJupyterBridge()\n            }\n        }\n    }\n\n    // Wait for request from Jupyter bridge\n    var jupyterBridgeURL = JupyterBridge + '/dequeue_request?channel=' + Channel\n    if (showDebug) {\n        console.log('Starting dequeue on Jupyter bridge: ' + jupyterBridgeURL)\n    }\n    httpJ.open('GET', jupyterBridgeURL, true)\n    httpJ.send()\n}\n\n// This kicks off a loop that ends by calling waitOnJupyterBridge again. This first call\n// ejects any dead readers before we start a read\nwaitOnJupyterBridge() // Wait for message from Jupyter bridge, execute it, and return reply\n\nif (showDebug) {\n    alert(\"Jupyter-bridge browser component is started on \" + JupyterBridge + ', channel ' + Channel)\n}\n",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#_PY4CYTOSCAPE = 'git+https://github.com/cytoscape/py4cytoscape@1.7.0' # optional\n",
        "import requests\n",
        "import py4cytoscape as p4c\n",
        "exec(requests.get(\"https://raw.githubusercontent.com/cytoscape/jupyter-bridge/master/client/p4c_init.py\").text)\n",
        "IPython.display.Javascript(_PY4CYTOSCAPE_BROWSER_CLIENT_JS) # Start browser client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANMVyLRAcM5b",
        "outputId": "26d65019-a898-4d84-bbf7-42b1c6d1ef9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'apiVersion': 'v1', 'cytoscapeVersion': '3.10.2', 'automationAPIVersion': '1.11.0', 'py4cytoscapeVersion': '1.11.0'}\n"
          ]
        }
      ],
      "source": [
        "# Get Cytoscape version information\n",
        "print(p4c.cytoscape_version_info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kvAiTv0oJtQ6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import py4cytoscape as p4c\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image as IPyImage\n",
        "import networkx as nx\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBsF4KW7KfLG",
        "outputId": "569b7a73-bf0f-4ea5-bb14-67d3a9220f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame head:\n",
            "    source   target\n",
            "0  DB00862  DB00966\n",
            "1  DB00575  DB00806\n",
            "2  DB01242  DB08893\n",
            "3  DB01151  DB08883\n",
            "4  DB01235  DB01275\n"
          ]
        }
      ],
      "source": [
        "# 1. Data Loading và Preprocessing\n",
        "data_path = r\"C:\\Users\\ADMIN\\Downloads\\ChCh-Miner_durgbank-chem-chem (1).tsv\"\n",
        "df = pd.read_csv(data_path, sep='\\t', names=[\"source\", \"target\"], header=None)\n",
        "# Display the DataFrame to verify\n",
        "print(\"DataFrame head:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame head:\n",
            "    source   target\n",
            "0  DB00862  DB00966\n",
            "1  DB00575  DB00806\n",
            "2  DB01242  DB08893\n",
            "3  DB01151  DB08883\n",
            "4  DB01235  DB01275\n",
            "Total nodes: 1514\n",
            "Number of nodes: 1514\n",
            "Number of edges: 48514\n",
            "Setting up training...\n",
            "Starting training...\n",
            "Epoch: 020, Loss: 0.6756, Train Acc: 0.5534, Val Acc: 0.4670\n",
            "Epoch: 040, Loss: 0.6137, Train Acc: 0.6223, Val Acc: 0.4361\n",
            "Epoch: 060, Loss: 0.5319, Train Acc: 0.7092, Val Acc: 0.4714\n",
            "Epoch: 080, Loss: 0.4703, Train Acc: 0.7658, Val Acc: 0.4890\n",
            "Epoch: 100, Loss: 0.4381, Train Acc: 0.7875, Val Acc: 0.5022\n",
            "Epoch: 120, Loss: 0.3979, Train Acc: 0.7970, Val Acc: 0.5198\n",
            "Epoch: 140, Loss: 0.3964, Train Acc: 0.7885, Val Acc: 0.4934\n",
            "Epoch: 160, Loss: 0.3923, Train Acc: 0.8045, Val Acc: 0.5022\n",
            "Epoch: 180, Loss: 0.3751, Train Acc: 0.8291, Val Acc: 0.5154\n",
            "Epoch: 200, Loss: 0.4389, Train Acc: 0.8036, Val Acc: 0.4978\n",
            "Epoch: 220, Loss: 0.3246, Train Acc: 0.8527, Val Acc: 0.4758\n",
            "Epoch: 240, Loss: 0.3075, Train Acc: 0.8489, Val Acc: 0.5022\n",
            "Epoch: 260, Loss: 0.3239, Train Acc: 0.8338, Val Acc: 0.5022\n",
            "Epoch: 280, Loss: 0.2951, Train Acc: 0.8706, Val Acc: 0.4890\n",
            "Epoch: 300, Loss: 0.3008, Train Acc: 0.8489, Val Acc: 0.4670\n",
            "Epoch: 320, Loss: 0.3554, Train Acc: 0.8281, Val Acc: 0.5110\n",
            "Epoch: 340, Loss: 0.3033, Train Acc: 0.8678, Val Acc: 0.4670\n",
            "Epoch: 360, Loss: 0.3679, Train Acc: 0.8640, Val Acc: 0.5022\n",
            "Epoch: 380, Loss: 0.2680, Train Acc: 0.8895, Val Acc: 0.4714\n",
            "Epoch: 400, Loss: 0.2890, Train Acc: 0.8744, Val Acc: 0.5022\n",
            "Epoch: 420, Loss: 0.3110, Train Acc: 0.8678, Val Acc: 0.4846\n",
            "Epoch: 440, Loss: 0.3110, Train Acc: 0.8678, Val Acc: 0.4581\n",
            "Epoch: 460, Loss: 0.2794, Train Acc: 0.8716, Val Acc: 0.4934\n",
            "Epoch: 480, Loss: 0.2751, Train Acc: 0.8848, Val Acc: 0.4934\n",
            "Epoch: 500, Loss: 0.2325, Train Acc: 0.8801, Val Acc: 0.5022\n",
            "\n",
            "Evaluating final model...\n",
            "\n",
            "Test Accuracy: 0.5395\n",
            "\n",
            "Generating embeddings...\n",
            "\n",
            "Applying PCA for dimensionality reduction...\n",
            "Applying t-SNE for dimensionality reduction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_31392\\3381416346.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pt'))\n",
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Converting edge_list_tuples to node indices...\n",
            "\n",
            "Splitting edges into train/validation/test sets for Link Prediction...\n",
            "\n",
            "Creating negative edges for training...\n"
          ]
        }
      ],
      "source": [
        "# Read the TSV file with no header and assign column names 'source' and 'target'\n",
        "df = pd.read_csv(data_path, sep='\\t', names=[\"source\", \"target\"], header=None)\n",
        "\n",
        "# Display the DataFrame to verify\n",
        "print(\"DataFrame head:\")\n",
        "print(df.head())\n",
        "\n",
        "# Tạo các danh sách các node duy nhất (source và target)\n",
        "unique_sources = df['source'].unique()\n",
        "unique_targets = df['target'].unique()\n",
        "unique_nodes = np.unique(np.concatenate((unique_sources, unique_targets)))\n",
        "\n",
        "# Tạo các mappings giữa node và chỉ số\n",
        "node_to_idx = {node: idx for idx, node in enumerate(unique_nodes)}\n",
        "idx_to_node = {idx: node for node, idx in node_to_idx.items()}\n",
        "\n",
        "# Tổng số node\n",
        "num_nodes = len(unique_nodes)\n",
        "print(f'Total nodes: {num_nodes}')\n",
        "\n",
        "# Tạo đồ thị đồng nhất bằng NetworkX\n",
        "G = nx.Graph()\n",
        "\n",
        "# Thêm các node\n",
        "for node in unique_nodes:\n",
        "    G.add_node(node)\n",
        "\n",
        "# Thêm các edge\n",
        "edge_list = df[['source', 'target']].values.tolist()\n",
        "G.add_edges_from(edge_list)\n",
        "\n",
        "# Kiểm tra số lượng node và edge\n",
        "print(f'Number of nodes: {G.number_of_nodes()}')\n",
        "print(f'Number of edges: {G.number_of_edges()}')\n",
        "\n",
        "# Tạo danh sách các edge dưới dạng chỉ số\n",
        "edge_index = torch.tensor([\n",
        "    [node_to_idx[source], node_to_idx[target]] for source, target in df.values\n",
        "], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Tạo node features (giả sử không có thuộc tính, sử dụng embedding ngẫu nhiên)\n",
        "hidden_dim = 64\n",
        "x = torch.randn(num_nodes, hidden_dim)  # Sử dụng embedding ngẫu nhiên\n",
        "\n",
        "# Tạo nhãn cho node (có thể sử dụng nhãn ngẫu nhiên hoặc dựa trên loại node)\n",
        "# Ví dụ với nhãn ngẫu nhiên:\n",
        "y = torch.randint(0, 2, (num_nodes,), dtype=torch.long)\n",
        "\n",
        "# Nếu muốn phân loại dựa trên loại node (source vs target), sử dụng đoạn mã sau:\n",
        "# labels = []\n",
        "# for node in unique_nodes:\n",
        "#     if node in unique_sources:\n",
        "#         labels.append(0)  # Label 0 cho source nodes\n",
        "#     else:\n",
        "#         labels.append(1)  # Label 1 cho target nodes\n",
        "# y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# Tạo đối tượng Data\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Chia dữ liệu thành train/val/test cho node classification\n",
        "train_idx, temp_idx = train_test_split(range(num_nodes), train_size=0.7, random_state=42)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "# Tạo masks\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "# Thêm masks vào đối tượng data\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# Model Definition\n",
        "class HomogeneousGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(HomogeneousGNN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.node_classifier = torch.nn.Linear(out_channels, 2)  # Đối với phân loại nhãn nhị phân\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index #Shape: (N, in_channels)\n",
        "        # GNN Layers\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x) #Shape: (N, hidden_channels)\n",
        "        x = self.conv2(x, edge_index) #Shape: (N, out_channels)\n",
        "        # Node Classification\n",
        "        out = self.node_classifier(x) #Shape: (N, 2)\n",
        "        return out, x  # Trả về cả dự đoán và embedding vectors\n",
        "\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out, _ = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data, mask, criterion=None):\n",
        "    model.eval()\n",
        "    out, embeddings = model(data)\n",
        "    if criterion:\n",
        "        loss = criterion(out[mask], data.y[mask]).item()\n",
        "    pred = out[mask].argmax(dim=1)\n",
        "    correct = (pred == data.y[mask]).sum().item()\n",
        "    acc = correct / mask.sum().item()\n",
        "    return acc\n",
        "\n",
        "print(\"Setting up training...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HomogeneousGNN(in_channels=hidden_dim, hidden_channels=128, out_channels=64).to(device)\n",
        "data = data.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Starting training...\")\n",
        "best_val_acc = 0\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(1, 501):\n",
        "    loss = train(model, data, optimizer, criterion)\n",
        "    if epoch % 20 == 0:\n",
        "        train_acc = evaluate(model, data, data.train_mask)\n",
        "        val_acc = evaluate(model, data, data.val_mask)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "# Đánh giá trên bộ test\n",
        "print(\"\\nEvaluating final model...\")\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "test_acc = evaluate(model, data, data.test_mask)\n",
        "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
        "\n",
        "print(\"\\nGenerating embeddings...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    _, embeddings = model(data)\n",
        "    embeddings = embeddings.cpu().numpy()\n",
        "\n",
        "# Áp dụng PCA -> giảm chiều\n",
        "print(\"\\nApplying PCA for dimensionality reduction...\")\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "embeddings_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "# Áp dụng t-SNE\n",
        "print(\"Applying t-SNE for dimensionality reduction...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "embeddings_tsne = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Chuẩn hóa các thành phần t-SNE để trực quan hóa Cytoscape\n",
        "scaler = MinMaxScaler()\n",
        "embeddings_tsne_norm = scaler.fit_transform(embeddings_tsne)\n",
        "\n",
        "# Tính độ lớn của các thành phần t-SNE\n",
        "tsne_magnitude = np.linalg.norm(embeddings_tsne_norm, axis=1)\n",
        "\n",
        "# Tạo DataFrame cho embeddings\n",
        "embeddings_df = pd.DataFrame(embeddings, columns=[f'embedding_{i}' for i in range(embeddings.shape[1])])\n",
        "embeddings_df['name'] = [idx_to_node[idx] for idx in range(num_nodes)]\n",
        "embeddings_df['pca_0'] = embeddings_pca[:, 0]\n",
        "embeddings_df['pca_1'] = embeddings_pca[:, 1]\n",
        "embeddings_df['tsne_0'] = embeddings_tsne[:, 0]\n",
        "embeddings_df['tsne_1'] = embeddings_tsne[:, 1]\n",
        "embeddings_df['tsne_magnitude'] = tsne_magnitude\n",
        "embeddings_df['label'] = y.cpu().numpy()\n",
        "\n",
        "# 17. Chuyển Đổi edge_list_tuples Thành Chỉ Số Node\n",
        "print(\"\\nConverting edge_list_tuples to node indices...\")\n",
        "edge_list_tuples = [(node_to_idx[u], node_to_idx[v]) for u, v in edge_list]\n",
        "\n",
        "# 18. Chia Cạnh Thành Train/Validation/Test Cho Link Prediction\n",
        "print(\"\\nSplitting edges into train/validation/test sets for Link Prediction...\")\n",
        "train_edges, test_edges = train_test_split(edge_list_tuples, test_size=0.2, random_state=42)\n",
        "val_edges, test_edges = train_test_split(test_edges, test_size=0.5, random_state=42)\n",
        "\n",
        "# 19. Tạo Negative Edges Cho Tập Train\n",
        "print(\"\\nCreating negative edges for training...\")\n",
        "num_negative_samples = len(train_edges)\n",
        "negative_edges = []\n",
        "while len(negative_edges) < num_negative_samples:\n",
        "    u, v = np.random.choice(num_nodes, size=2, replace=False)\n",
        "    if (u, v) not in G.edges() and (v, u) not in G.edges():  # Đảm bảo cạnh âm không tồn tại\n",
        "        negative_edges.append((u, v))\n",
        "\n",
        "import torch.nn as nn\n",
        "# 20. Định Nghĩa Link Predictor Model\n",
        "print(\"\\nDefining the Link Predictor model...\")\n",
        "class LinkPredictor(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "        self.linear = nn.Linear(embedding_dim * 2, 1)  # Cho phép nối embeddings\n",
        "\n",
        "    def forward(self, node1_embedding, node2_embedding):\n",
        "        combined_embedding = torch.cat([node1_embedding, node2_embedding], dim=1)\n",
        "        output = torch.sigmoid(self.linear(combined_embedding))  # Dự đoán xác suất liên kết\n",
        "        return output\n",
        "\n",
        "# 21. Tạo Link Predictor và Định Nghĩa Optimizer & Criterion\n",
        "print(\"\\nSetting up Link Predictor training...\")\n",
        "embedding_dim = embeddings.shape[1]\n",
        "link_predictor = LinkPredictor(embedding_dim).to(device)\n",
        "optimizer_lp = torch.optim.Adam(link_predictor.parameters(), lr=0.01)\n",
        "criterion_lp = nn.BCELoss()\n",
        "\n",
        "# 22. Định Nghĩa Hàm Để Lấy Embeddings Cho Các Cạnh\n",
        "def get_edge_embeddings(edges, embeddings):\n",
        "    # Tách các chỉ số nguồn và đích từ danh sách cạnh\n",
        "    src = [edge[0] for edge in edges]\n",
        "    dst = [edge[1] for edge in edges]\n",
        "    src = np.array(src)\n",
        "    dst = np.array(dst)\n",
        "    src_embeddings = embeddings[src]\n",
        "    dst_embeddings = embeddings[dst]\n",
        "    return src_embeddings, dst_embeddings\n",
        "\n",
        "# 23. Định Nghĩa Hàm Để Tạo Labels Cho Link Prediction\n",
        "def get_link_labels(positive_edges, negative_edges):\n",
        "    # Tạo labels: 1 cho positive edges, 0 cho negative edges\n",
        "    labels = torch.cat([torch.ones(len(positive_edges)), torch.zeros(len(negative_edges))], dim=0).to(device)\n",
        "    return labels\n",
        "\n",
        "# 24. Định Nghĩa Hàm Để Đánh Giá Link Prediction\n",
        "@torch.no_grad()\n",
        "def evaluate_link_prediction(model, link_predictor, edges, embeddings, criterion):\n",
        "    model.eval()  # Đảm bảo mô hình GNN không được cập nhật trong quá trình đánh giá\n",
        "    link_predictor.eval()\n",
        "    src_embeddings, dst_embeddings = get_edge_embeddings(edges, embeddings)\n",
        "    src_embeddings = torch.tensor(src_embeddings, dtype=torch.float32).to(device)\n",
        "    dst_embeddings = torch.tensor(dst_embeddings, dtype=torch.float32).to(device)\n",
        "    predictions = link_predictor(src_embeddings, dst_embeddings).squeeze()\n",
        "    loss = criterion(predictions, torch.ones_like(predictions)).item()\n",
        "    return loss\n",
        "\n",
        "# 25. Định Nghĩa Hàm Huấn Luyện Link Predictor\n",
        "def train_link_prediction(model, link_predictor, optimizer, criterion, train_edges, negative_edges, embeddings):\n",
        "    model.eval()  # Đảm bảo mô hình GNN không được cập nhật trong quá trình huấn luyện link predictor\n",
        "    link_predictor.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Lấy embeddings cho các cạnh dương (positive edges)\n",
        "    train_src_embeddings, train_dst_embeddings = get_edge_embeddings(train_edges, embeddings)\n",
        "    train_src_embeddings = torch.tensor(train_src_embeddings, dtype=torch.float32).to(device)\n",
        "    train_dst_embeddings = torch.tensor(train_dst_embeddings, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Lấy embeddings cho các cạnh âm (negative edges)\n",
        "    neg_src_embeddings, neg_dst_embeddings = get_edge_embeddings(negative_edges, embeddings)\n",
        "    neg_src_embeddings = torch.tensor(neg_src_embeddings, dtype=torch.float32).to(device)\n",
        "    neg_dst_embeddings = torch.tensor(neg_dst_embeddings, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Dự đoán liên kết\n",
        "    pos_pred = link_predictor(train_src_embeddings, train_dst_embeddings)\n",
        "    neg_pred = link_predictor(neg_src_embeddings, neg_dst_embeddings)\n",
        "\n",
        "    # Kết hợp predictions và labels\n",
        "    predictions = torch.cat([pos_pred, neg_pred], dim=0).squeeze()\n",
        "    labels = get_link_labels(train_edges, negative_edges)\n",
        "\n",
        "    # Tính toán loss\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# 26. Huấn Luyện Link Predictor\n",
        "print(\"\\nStarting Link Prediction training...\")\n",
        "epochs_lp = 100\n",
        "best_val_loss_lp = float('inf')\n",
        "\n",
        "for epoch in range(1, epochs_lp + 1):\n",
        "    loss = train_link_prediction(model, link_predictor, optimizer_lp, criterion_lp, train_edges, negative_edges, embeddings)\n",
        "    val_loss = evaluate_link_prediction(model, link_predictor, val_edges, embeddings, criterion_lp)\n",
        "    print(f'Epoch [{epoch}/{epochs_lp}], Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss_lp:\n",
        "        best_val_loss_lp = val_loss\n",
        "        best_epoch_lp = epoch\n",
        "        torch.save(link_predictor.state_dict(), 'best_link_prediction_model.pth')\n",
        "\n",
        "# 27. Đánh Giá Link Predictor Trên Tập Test\n",
        "print(\"\\nEvaluating Link Predictor on test set...\")\n",
        "link_predictor.load_state_dict(torch.load('best_link_prediction_model.pth'))\n",
        "test_loss_lp = evaluate_link_prediction(model, link_predictor, test_edges, embeddings, criterion_lp)\n",
        "print(f'Test Loss: {test_loss_lp:.4f}')\n",
        "\n",
        "# 28. Thêm Thuộc Tính Link Prediction Vào Edge Table\n",
        "print(\"\\nAdding Link Prediction results to edge table...\")\n",
        "\n",
        "# Tạo DataFrame cho các cạnh gốc và cạnh dự đoán\n",
        "existing_edge_df = pd.DataFrame(edge_list_tuples, columns=['source', 'target'])\n",
        "existing_edge_df['prediction'] = 'existing'\n",
        "\n",
        "# --- Điều Chỉnh Để Giới Hạn Số Lượng Cạnh Dự Đoán ---\n",
        "# Thay vì dự đoán tất cả các cạnh không tồn tại, chúng ta đã chọn một tập hợp ngẫu nhiên và giới hạn số lượng cạnh cần dự đoán (100,000)\n",
        "# Tuy nhiên, chúng ta sẽ chỉ chọn top 100 cạnh dự đoán để tránh sử dụng quá nhiều RAM và tạo thêm các cạnh mới trong Cytoscape.\n",
        "\n",
        "print(\"\\nCollecting Link Prediction results with limited candidate edges to avoid RAM crash...\")\n",
        "# Giới hạn số lượng cạnh cần dự đoán (ví dụ: 100,000)\n",
        "max_candidate_edges = 100000\n",
        "\n",
        "# Tạo danh sách các cạnh không tồn tại\n",
        "# Chọn ngẫu nhiên một tập hợp các cạnh không tồn tại\n",
        "candidate_edges = set()\n",
        "while len(candidate_edges) < max_candidate_edges:\n",
        "    u = np.random.randint(0, num_nodes)\n",
        "    v = np.random.randint(0, num_nodes)\n",
        "    if u != v and (u, v) not in G.edges() and (v, u) not in G.edges():\n",
        "        candidate_edges.add((u, v))\n",
        "    if len(candidate_edges) % 10000 == 0 and len(candidate_edges) > 0:\n",
        "        print(f\"{len(candidate_edges)} candidate edges collected...\")\n",
        "\n",
        "candidate_edges = list(candidate_edges)\n",
        "print(f\"Total candidate edges for prediction: {len(candidate_edges)}\")\n",
        "\n",
        "# Lấy embeddings cho các cạnh dự đoán\n",
        "src_embeddings, dst_embeddings = get_edge_embeddings(candidate_edges, embeddings)\n",
        "src_embeddings = torch.tensor(src_embeddings, dtype=torch.float32).to(device)\n",
        "dst_embeddings = torch.tensor(dst_embeddings, dtype=torch.float32).to(device)\n",
        "\n",
        "# Dự đoán xác suất liên kết\n",
        "print(\"\\nPredicting link probabilities...\")\n",
        "with torch.no_grad():\n",
        "    predictions = link_predictor(src_embeddings, dst_embeddings).squeeze().cpu().numpy()\n",
        "\n",
        "# Chọn các cạnh có xác suất dự đoán cao nhất (ví dụ: top 100)\n",
        "top_k = 100\n",
        "top_k_indices = np.argsort(predictions)[-top_k:]\n",
        "predicted_edges = [candidate_edges[i] for i in top_k_indices]\n",
        "predicted_scores = predictions[top_k_indices]\n",
        "\n",
        "print(f\"Top {top_k} predicted edges selected.\")\n",
        "\n",
        "# Tạo DataFrame cho các cạnh dự đoán\n",
        "predicted_edge_df = pd.DataFrame(predicted_edges, columns=['source', 'target'])\n",
        "predicted_edge_df['prediction'] = 'predicted'\n",
        "predicted_edge_df['score'] = predicted_scores\n",
        "\n",
        "# Kết hợp DataFrame cho các cạnh gốc và cạnh dự đoán\n",
        "combined_edge_df = pd.concat([existing_edge_df, predicted_edge_df[['source', 'target', 'prediction', 'score']]], ignore_index=True)\n",
        "\n",
        "# 29. Tạo Mạng Lưới Trong Cytoscape với Cả Các Cạnh Dự Đoán\n",
        "print(\"\\nCreating network in Cytoscape using NetworkX with predicted edges...\")\n",
        "# Tạo đồ thị NetworkX với các node và edge\n",
        "G_cytoscape = nx.Graph()\n",
        "\n",
        "# Thêm các node với thuộc tính 'name'\n",
        "for node in unique_nodes:\n",
        "    G_cytoscape.add_node(node, name=node)\n",
        "\n",
        "# Thêm các cạnh hiện có với thuộc tính 'prediction' = 'existing'\n",
        "for u, v in edge_list:\n",
        "    G_cytoscape.add_edge(u, v, prediction='existing')\n",
        "\n",
        "# Thêm các cạnh dự đoán với thuộc tính 'prediction' = 'predicted' và 'score'\n",
        "for idx, row in predicted_edge_df.iterrows():\n",
        "    u_idx, v_idx = row['source'], row['target']\n",
        "    u_node, v_node = idx_to_node[u_idx], idx_to_node[v_idx]\n",
        "    score = row['score']\n",
        "    G_cytoscape.add_edge(u_node, v_node, prediction='predicted', score=score)\n",
        "\n",
        "# Tạo mạng lưới trong Cytoscape từ NetworkX\n",
        "network_title = 'Drugs Interaction Network'\n",
        "network_suid = p4c.create_network_from_networkx(G_cytoscape, title=network_title)\n",
        "print(f\"Network created with SUID: {network_suid}\")\n",
        "\n",
        "# Chờ mạng lưới được tạo thành công\n",
        "print(\"Waiting for the network to be created in Cytoscape...\")\n",
        "time.sleep(5)  # Chờ 5 giây để đảm bảo mạng lưới đã được tải\n",
        "\n",
        "# 30. Tải Các Thuộc Tính Embedding vào Cytoscape\n",
        "print(\"\\nLoading PCA and t-SNE data into Cytoscape...\")\n",
        "# Tải thuộc tính PCA\n",
        "p4c.load_table_data(\n",
        "    data=embeddings_df[['name', 'pca_0', 'pca_1']],\n",
        "    data_key_column='name',\n",
        "    table='node',\n",
        "    table_key_column='name',\n",
        "    network=network_title\n",
        ")\n",
        "\n",
        "# Tải thuộc tính t-SNE\n",
        "p4c.load_table_data(\n",
        "    data=embeddings_df[['name', 'tsne_0', 'tsne_1', 'tsne_magnitude']],\n",
        "    data_key_column='name',\n",
        "    table='node',\n",
        "    table_key_column='name',\n",
        "    network=network_title\n",
        ")\n",
        "\n",
        "# 31. Định Nghĩa và Áp Dụng Visual Style\n",
        "print(\"\\nDefining and applying visual styles...\")\n",
        "# Tạo một visual style mới\n",
        "style_name = \"GNN_Homogeneous_Style\"\n",
        "if style_name not in p4c.get_visual_style_names():\n",
        "    p4c.styles.create_visual_style(style_name)\n",
        "    print(f\"Visual style '{style_name}' created.\")\n",
        "else:\n",
        "    print(f\"Visual style '{style_name}' already exists.\")\n",
        "\n",
        "# Áp dụng visual style\n",
        "p4c.set_visual_style(style_name)\n",
        "print(f\"Visual style '{style_name}' applied.\")\n",
        "\n",
        "# Thêm nhãn vào DataFrame và tải vào Cytoscape\n",
        "embeddings_df['label'] = y.cpu().numpy()\n",
        "\n",
        "# Tải thuộc tính 'label' vào Cytoscape node table\n",
        "print(\"\\nLoading label data into Cytoscape node table...\")\n",
        "p4c.load_table_data(\n",
        "    data=embeddings_df[['name', 'label']],\n",
        "    data_key_column='name',\n",
        "    table='node',\n",
        "    table_key_column='name',\n",
        "    network=network_title  # Sử dụng biến tên mạng lưới đã định nghĩa\n",
        ")\n",
        "print(\"'label' column loaded to Cytoscape node table.\")\n",
        "\n",
        "# Ánh xạ màu sắc cho node dựa trên 'label'\n",
        "print(\"Mapping node color based on label...\")\n",
        "# Định nghĩa palette cho hai nhãn bằng cyPalette và chỉ lấy hai màu đầu tiên\n",
        "palette_colors = p4c.cyPalette(name='set1')[:2]  # Label 0: màu đầu tiên, Label 1: màu thứ hai\n",
        "palette = ('custom_palette', 'qualitative', lambda n: palette_colors)\n",
        "print(f\"Palette used for mapping: {palette_colors}\")\n",
        "\n",
        "# Tạo color map cho node dựa trên 'label'\n",
        "label_color_map = p4c.gen_node_color_map(\n",
        "    table_column='label',\n",
        "    palette=palette,          # Sử dụng palette đã định nghĩa\n",
        "    mapping_type='d',         # 'd' cho discrete mapping\n",
        "    default_color='#CCCCCC',  # Màu xám nhạt mặc định cho các nhãn không xác định\n",
        "    style_name=style_name,    # Tên style đã tạo trước đó\n",
        "    network=network_title     # Đảm bảo tên mạng lưới chính xác\n",
        ")\n",
        "\n",
        "# Áp dụng color mapping\n",
        "p4c.set_node_color_mapping(**label_color_map)\n",
        "print(\"Node color mapping based on label applied.\")\n",
        "\n",
        "# Ánh xạ 'pca_0' vào kích thước node\n",
        "print(\"Mapping PCA component to node size...\")\n",
        "size_map_pca = p4c.gen_node_size_map(\n",
        "    table_column='pca_0',\n",
        "    mapping_type='c',          # 'c' cho continuous mapping\n",
        "    style_name=style_name,\n",
        "    network=network_title      # Sử dụng biến tên mạng lưới đã định nghĩa\n",
        ")\n",
        "\n",
        "p4c.set_node_size_mapping(**size_map_pca)\n",
        "print(\"Node size mapping based on 'pca_0' applied.\")\n",
        "\n",
        "# Ánh xạ 'name' vào nhãn node\n",
        "print(\"Mapping node labels...\")\n",
        "p4c.style_mappings.set_node_label_mapping('name', style_name=style_name)\n",
        "\n",
        "# Áp dụng visual style\n",
        "p4c.set_visual_style(style_name)\n",
        "\n",
        "# 32. Định Nghĩa và Áp Dụng Edge Style Mapping Generators\n",
        "print(\"\\nDefining and applying edge style mappings for Link Prediction...\")\n",
        "\n",
        "# Ánh xạ màu sắc cho cạnh dựa trên thuộc tính 'prediction' sử dụng một palette có sẵn\n",
        "print(\"Mapping edge color based on prediction status using a predefined palette...\")\n",
        "color_map = p4c.gen_edge_color_map(\n",
        "    table_column='prediction',\n",
        "    palette=p4c.palette_color_brewer_q_Set1(),  # Sử dụng hàm palette có sẵn\n",
        "    mapping_type='d',                          # 'd' cho discrete mapping\n",
        "    default_color='#000000',                   # Màu đen mặc định\n",
        "    style_name=style_name,\n",
        "    network=network_suid\n",
        ")\n",
        "p4c.set_edge_color_mapping(**color_map)\n",
        "\n",
        "# Ánh xạ kiểu đường kẻ cho cạnh dựa trên thuộc tính 'prediction'\n",
        "print(\"Mapping edge line style based on prediction status...\")\n",
        "line_style_map = p4c.gen_edge_line_style_map(\n",
        "    table_column='prediction',\n",
        "    default_line_style='SOLID',                # Mặc định là SOLID\n",
        "    style_name=style_name,\n",
        "    network=network_suid\n",
        ")\n",
        "p4c.set_edge_line_style_mapping(**line_style_map)\n",
        "\n",
        "# Áp dụng visual style\n",
        "p4c.set_visual_style(style_name)\n",
        "\n",
        "# Áp dụng layout Force-Directed để sắp xếp mạng lưới\n",
        "print(\"Applying Force-Directed layout...\")\n",
        "p4c.layout_network('force-directed', network=network_suid)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
